{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSCI 347\n",
    "Project 3\n",
    "Names: Moiyad Alfawwar, Michael Roduin, Philip Ghede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Think about the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set comes from comes from the daily measures of sensors in an urban waste water treatment plant. The data helps us classify the operational state of the plant in order to predict faults through the state variables of the plant at each of the stages of the treatment process.\n",
    "\n",
    "This multivariate data set is interesting because of the importance of predicting failures in the operational state of treatment plants and the likes through data analysis, as a means to avoid ecological catastrophes resulting from possible failures. We are also interested in determining how difficult it is to notice fluctuations in performance and output, in order to predict future failures and ensure quality of output. \n",
    "\n",
    "This data set has 527 instances or observations and 38 Attributes. We are not missing any values so we will not be using any techniques (discussed in previous projects) to account for missing values, unless missing values are later discovered. A lack of missing data is not surprising because we expect a complex piece of machinery like a water treatment plant to carefully track data and have this readily available for quality insurance.  \n",
    "\n",
    "\n",
    "We can use many clustering techniques to evaluate this data including K-Means, and DBSCAN, and expect a minimum of 3 clusters, up to the 13, based on the class distribution described in data set description. Upon analysis, resulting clusters will allow us to determine deviation in performance in general and when influenced by varying conditions. Clustering is an unsupervised machine learning method used for grouping similar data points. With the help of clustering, we can classify data into structures that can be evaluated and manipulated easily. Given our data set, we might find it useful to identify clusters based on varying input, output, or performance to determine possible risk factors, etc. We expect to find many clusters of varying size and intensity depending on our attributes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Write Python code for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_boston\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Analyze your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (4 points) Use sklearn’s PCA implementation to linearly transform the data to two dimensions.\n",
    "Create a scatter plot of the data, with the x-axis corresponding to coordinates of the data\n",
    "along the first principal component, and the y-axis corresponding to coordinates of the data\n",
    "along the second principal component. Does it look like there are clusters in these two\n",
    "dimensions? If so, how many would you say there are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = np.genfromtxt('data/water-treatment.data', delimiter=\",\")\n",
    "pca = PCA(n_components=2)\n",
    "Data2 = np.delete(Data, [0],1)\n",
    "D3 = Data2[:, ~np.isnan(Data2).any(axis=0)]\n",
    "pca_data = pca.fit_transform(D3)\n",
    "# Data2\n",
    "pca_data\n",
    "\n",
    "plt.scatter(pca_data[:,0],pca_data[:,1])\n",
    "plt.xlabel('principal component 1')\n",
    "plt.ylabel('principle component 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (3 points) Use sklearn’s PCA implementation to linearly transform the data, without spec-\n",
    "ifying the number of components to use. Create a plot with r, the number of components\n",
    "(i.e., dimensionality), on the x-axis, and f (r), the fraction of total variance captured in the\n",
    "first r principal components, on the y-axis. Based on this plot, choose a number of principal\n",
    "components to reduce the dimensionality of the data. Report how many principal components\n",
    "will be used as well as the faction of total variance captured using this many components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA()\n",
    "pca_data2 = pca2.fit_transform(D3)\n",
    "\n",
    "plt.plot(range(1,8),np.cumsum(pca2.explained_variance_ratio_), marker='x')\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('fraction of total variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (5 points) For both the original and the reduced-dimensionality data obtained using PCA in\n",
    "question 3.2, do the following: Experiment with a range of values for the number of clusters,\n",
    "k, that you pass as input to the k-means function, to find clusters in the chosen data set. Use\n",
    "at least 5 different values of k. For each value of k, report the value of the objective function\n",
    "for that choice of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans1 = KMeans(n_clusters=3, init='k-means++', max_iter=300, random_state=0)\n",
    "kmeans2 = KMeans(n_clusters=4, init='k-means++', max_iter=300, random_state=0)\n",
    "kmeans3 = KMeans(n_clusters=5, init='k-means++', max_iter=300, random_state=0)\n",
    "kmeans4 = KMeans(n_clusters=6, init='k-means++', max_iter=300, random_state=0)\n",
    "kmeans5 = KMeans(n_clusters=7, init='k-means++', max_iter=300, random_state=0)\n",
    "\n",
    "\n",
    "pred_labels1 = kmeans1.fit_predict(D3)\n",
    "pred_labels_pca1 = kmeans1.fit_predict(pca_data2)\n",
    "pred_labels2 = kmeans2.fit_predict(D3)\n",
    "pred_labels_pca2 = kmeans2.fit_predict(pca_data2)\n",
    "pred_labels3 = kmeans3.fit_predict(D3)\n",
    "pred_labels_pca3 = kmeans3.fit_predict(pca_data2)\n",
    "pred_labels4 = kmeans4.fit_predict(D3)\n",
    "pred_labels_pca4 = kmeans4.fit_predict(pca_data2)\n",
    "pred_labels5 = kmeans5.fit_predict(D3)\n",
    "pred_labels_pca5 = kmeans5.fit_predict(pca_data2)\n",
    "pred_labels1[:300]\n",
    "pred_labels_pca1[:100]\n",
    "\n",
    "pred_labels_pca5[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (5 points) For both the original and the reduced-dimensionality data obtained using PCA in\n",
    "question 3.2, do the following: Experiment with a range of values for the minpts and $\\epsilon$ input\n",
    "parameters to the DBSCAN function to find clusters in the chosen data set. First, keep $\\epsilon$\n",
    "fixed and try out a range of different values for minpts. Then keep minpts fixed, and try a\n",
    "range of values for $\\epsilon$. Use at least 5 values of $\\epsilon$ and at least 5 values of minpts. Report the\n",
    "number of clusters found for each (minpts, $\\epsilon$) pair tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs1 = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbs2 = DBSCAN(eps=0.5, min_samples=6)\n",
    "dbs3 = DBSCAN(eps=0.5, min_samples=7)\n",
    "dbs4 = DBSCAN(eps=0.5, min_samples=8)\n",
    "dbs5 = DBSCAN(eps=0.5, min_samples=9)\n",
    "dbs_e1 = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbs_e2 = DBSCAN(eps=0.7, min_samples=5)\n",
    "dbs_e3 = DBSCAN(eps=0.9, min_samples=5)\n",
    "dbs_e4 = DBSCAN(eps=1.1, min_samples=5)\n",
    "dbs_e5 = DBSCAN(eps=1.3, min_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. (Extra credit - 3 points): Create a plot of clustering precision for each value of k used in\n",
    "question 3.3, each value of $\\epsilon$ used in question 3.4, and each value of minpts used in question\n",
    "3.4, for both the original and reduced-dimensionality data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "435ca20ebd40facca939da61bf2b33486b57b976545d40e14cf63d5fdaaad0da"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('csci347')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}