{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Plan  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; In this project we are investigating the correlation between various risk factors such as cholesterol, high-blood pressure, etc. and heart disease. The multivariate heart Disease Data Set we are using contains 270 instances, with 13 attributes that give us insight into the observed patients' biology and lifestyle. The Dataset is said to include Categorical values, Integer, and Real numbers. Some values are missing, so the data will need to be cleaned. \n",
    "\n",
    "&emsp;&emsp; This dataset has been cited in several research papers in the data science field including: Diversity in Neural Network Ensembles (Gavin Brown. The University of Birmingham. 2004.), Overcoming the Myopia of Inductive Learning Algorithms with RELIEFF (Igor Kononenko and Edvard Simec and Marko Robnik-Sikonja), Unanimous Voting using Support Vector Machines (Elena Smirnova and Ida G. Sprinkhuizen-Kuyper and I. Nalbantis and b. ERIM and Universiteit Rotterdam, IKAT, Universiteit Maastricht), Dissertation Towards Understanding Stacking Studies of a General Ensemble Learning Scheme ausgef√ºhrt zum Zwecke der Erlangung des akademischen Grades eines Doktors der technischen Naturwissenschaften.\n",
    "\n",
    "&emsp;&emsp; The dataset was uploaded by the University of California, Irvine, and is available at the UCI machine learning archive on their website for the Center for Machine Learning and Intelligent Systems at the following URL: https://archive.ics.uci.edu/ml/datasets/statlog+(heart)\n",
    "\n",
    "&emsp;&emsp; Our data set has already been processed and includes no missing values, so there is no need for a plot to summarize the proportion of missing data as this is non-applicable for all 13 attributes. The 13 attributes (which have been extracted from a larger set of 75) include descriptive variables such as sex, and Chest pain type (4 values), however, these values were already label encoded. As such, we are left with the following attribute types: Real: 1,4,5,8,10,12 Ordered:11,Binary: 2,6,9 Nominal:7,3,13\n",
    "\n",
    "&emsp;&emsp; Given that the categorical values were already label encoded, the choice (label-encoded vs one hot-encoded) was made for us and we assume that the alphabetical ordering of label encoding will not prevent us from making medically relevant inferences from this data. In other words, we assume that the categorical value was ordered alphabetically as to represent the severity of the pain, for example, A-D. This assumption may give us trouble down the road, and should be further investigated.\n",
    "\n",
    "&emsp;&emsp; If we were to work with categorical values for chest pain, one might suggest one-hot encoding to prevent any issues that may arise if there is no obvious ordering, or ranking of our values, and rather solve this potential problem by representing each category as a binary vector. However, in order to avoid the pitfalls of multicollinearity, it would be best to simply determine that categorical data is ranked appropriately and use label-encoding instead. Sex/Gender is binary, and so labelencoding can be considered appropriate.\n",
    "\n",
    "&emsp;&emsp; We will be using k-means clustering to help identify variables that are most correlated to heart disease, and use dimensionality reduction across all attributes to determine the correlation between respective attributes and k-means.These techniques belong to the class of unsupervised learning and represent highly relevant skills in the toolbelt of every data miner, allowing us to demonstrate our progress throughout the course. At this point we are more concerned with descriptive analysis using the tools that we learned in data mining.    \n",
    "\n",
    "If we were to continue our research into this data set we might concern ourselves with more predictive analysis using tools like classification and regression models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "\n",
    "# Implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_boston\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import random\n",
    "import networkx as nx\n",
    "from scipy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "hearts = np.loadtxt(\"./data/heart.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for implemnetation code from previous projects code\n",
    "\n",
    "# Function for the multivariate mean of a numerical data set\n",
    "def twoDimMean(inputArr):\n",
    "    total = np.zeros([len(inputArr[0])])\n",
    "    num_elements = len(inputArr)\n",
    "\n",
    "    for i in range(len(inputArr)):\n",
    "        for j in range(len(inputArr[0])):\n",
    "            xi = inputArr[:, j]\n",
    "            total[j] = total[j] + xi[i]\n",
    "\n",
    "    mean = total/num_elements\n",
    "    return mean\n",
    "\n",
    "\n",
    "# Function for sample covariance between two attributes\n",
    "def covariance(a, b):\n",
    "    x, y, total = 0, 0, 0\n",
    "    n = len(a)\n",
    "    # attr. Sum \n",
    "    for i in range(n):\n",
    "        x = x + a[i]\n",
    "        y = y + b[i]\n",
    "    # attr. Mean\n",
    "    xMean = x / n\n",
    "    yMean = y / n\n",
    "\n",
    "    # Covariance sum\n",
    "    for i in range(n):\n",
    "        total = total + ((a[i]) - xMean) * ((b[i]) - yMean)\n",
    "    covariance = (1 / (n - 1)) * total\n",
    "    return covariance\n",
    "\n",
    "\n",
    "\n",
    "# Function for correlation between two attributes\n",
    "def correlation(p1, p2):\n",
    "    x, y, axb, a2, b2 = 0, 0, 0, 0, 0\n",
    "    n = len(p1)\n",
    "\n",
    "    for i in range(n):\n",
    "        x = x + p1[i]\n",
    "        y = y + p2[i]\n",
    "\n",
    "    x_mean = x / n\n",
    "    y_mean = y / n\n",
    "\n",
    "    for i in range(n):\n",
    "        a = (p1[i] - x_mean)\n",
    "        b = (p2[i] - y_mean)\n",
    "        axb = axb + (a * b)\n",
    "        a2 = a2 + (a * a)\n",
    "        b2 = b2 + (b * b)\n",
    "    \n",
    "    correlation = axb / (math.sqrt(a2 * b2))\n",
    "    \n",
    "    return correlation\n",
    "\n",
    "# Function for min by column\n",
    "# ref: https://www.geeksforgeeks.org/program-find-minimum-maximum-element-array/\n",
    "def getMin(inputArr):\n",
    "    value = inputArr[0]\n",
    "    n = len(inputArr)\n",
    "    for i in range(1, n):\n",
    "        value = min(value, inputArr[i])\n",
    "    return value\n",
    "\n",
    "\n",
    "# Function for max by column\n",
    "# ref: https://www.geeksforgeeks.org/program-find-minimum-maximum-element-array/\n",
    "def getMax(inputArr):\n",
    "    value = inputArr[0]\n",
    "    n = len(inputArr)\n",
    "    for i in range(1, n):\n",
    "        value = max(value, inputArr[i])\n",
    "    return value\n",
    "\n",
    "def rangeNormal(inputArr):\n",
    "    rowLen = len(inputArr)\n",
    "    colLen = len(inputArr[0])\n",
    "    rngNormal = np.empty([rowLen, colLen])\n",
    "    \n",
    "    for row in range(rowLen):\n",
    "        for col in range(colLen):\n",
    "            max_min = getMax(inputArr[:, col]) - getMin(inputArr[:, col])\n",
    "            x_sub_min = (inputArr[row][col] - getMin(inputArr[:, col]))\n",
    "            rngNormal[row][col] = x_sub_min / max_min\n",
    "    return rngNormal\n",
    "\n",
    "    # Helper function \n",
    "def oneDimMean(inputArr):\n",
    "    total = 0\n",
    "    numElements = 0\n",
    "\n",
    "    for i in range(len(inputArr)):\n",
    "        total = total + inputArr[i]\n",
    "        numElements = numElements + 1\n",
    "    mean = total / numElements\n",
    "    return mean\n",
    "\n",
    "\n",
    "# Helper function for SD\n",
    "def stdDevHelper(inputArr):\n",
    "    total = 0\n",
    "    numElements = 0\n",
    "    mean = oneDimMean(inputArr)\n",
    "\n",
    "    for i in range(len(inputArr)):\n",
    "        total = total + (inputArr[i] - mean) ** 2\n",
    "        numElements = numElements + 1\n",
    "\n",
    "    std_dev = math.sqrt(total / (numElements - 1))\n",
    "    return std_dev\n",
    "\n",
    "\n",
    "def listUnique(inputArr):\n",
    "    unique = []\n",
    "    for x in inputArr:\n",
    "        if x not in unique:\n",
    "            unique.append(x)\n",
    "    return unique\n",
    "\n",
    "\n",
    "def stdNormal(inputArr):\n",
    "    rowLen = len(inputArr)\n",
    "    colLen = len(inputArr[0])\n",
    "\n",
    "    stdNormal = np.empty([rowLen, colLen])\n",
    "    for i in range(rowLen):\n",
    "        for j in range(colLen):\n",
    "            xi = inputArr[i][j]\n",
    "            colMean = oneDimMean(inputArr[:, j])\n",
    "            stdDev = stdDevHelper(inputArr[:, j])\n",
    "            stdNormal[i][j] = (xi - colMean) / stdDev\n",
    "    return stdNormal\n",
    "\n",
    "\n",
    "def covarMatrix(inputArr):\n",
    "    # dim = np.array = inputArr.shape\n",
    "    rowLen = inputArr.shape[1]\n",
    "    \n",
    "    covarMatrix = np.empty([rowLen, rowLen])\n",
    "    for i in range(rowLen):\n",
    "        for j in range(rowLen):\n",
    "            xi = inputArr[:, i]\n",
    "            xj = inputArr[:, j]\n",
    "            covarMatrix[i,j] = covariance(xi, xj)\n",
    "    return covarMatrix\n",
    "\n",
    "\n",
    "def labelEncode(inputArr):\n",
    "    rowLen = len(inputArr)\n",
    "    colLen = len(inputArr[0])\n",
    "\n",
    "    lblEncoded = np.empty([rowLen, colLen])\n",
    "    for i in range(rowLen):\n",
    "        for j in range(colLen):\n",
    "            xi = inputArr[:, j]\n",
    "            xu = listUnique(xi)\n",
    "            lblEncoded[i][j] = xu.index(xi[i]) + 1\n",
    "\n",
    "    return lblEncoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Function to determine number of vertices in a graph\n",
    "def number_of_vertices(graph):\n",
    "    # tuples to list\n",
    "    my_list = [item for x in graph for item in x]\n",
    "    # casting to set to remove dublicate vals.\n",
    "    my_set = set(my_list)\n",
    "    counter = len(my_set)\n",
    "    return counter\n",
    "\n",
    "\n",
    "# 2.2 Function for finding the degree of a vertex\n",
    "def degrees_of_vertex(graph, vertex):\n",
    "    # Converting tuples to list\n",
    "    my_list = [item for x in graph for item in x]\n",
    "    counter = 0\n",
    "    for i in my_list:\n",
    "        if i == vertex:\n",
    "            counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "# number of edges from a subgraph\n",
    "def subgraph_num_edges(graph, vertices):\n",
    "    Graph = nx.Graph()\n",
    "    Graph.add_edges_from(graph)\n",
    "    induced_subgraph = Graph.subgraph(vertices)\n",
    "    number_of_edges = nx.number_of_edges(induced_subgraph)\n",
    "    return number_of_edges\n",
    "\n",
    "\n",
    "# Creates the graph\n",
    "def create_graph(edges_of_graph):\n",
    "    Graph = nx.Graph()\n",
    "    Graph.add_edges_from(edges_of_graph)\n",
    "    return Graph\n",
    "\n",
    "# Clustering Coefficient\n",
    "def clustering_coefficient(graph, vertex):\n",
    "    # Obtains a list of all the edges of the given vertex\n",
    "    f_list = list(filter(lambda x: vertex in x, graph))\n",
    "    # Converts the list of tuples into a regular list\n",
    "    my_list = [item for x in f_list for item in x]\n",
    "    # __ne__ is 'not equal' paramter.\n",
    "    edges = list(filter(vertex.__ne__, my_list))\n",
    "    current_total_edges = subgraph_num_edges(edges, graph)\n",
    "    number_of_edges = len(edges)\n",
    "    maximum_total_edges = (number_of_edges - 1) * (number_of_edges / 2)\n",
    "    clustering = current_total_edges / maximum_total_edges\n",
    "    return clustering\n",
    "\n",
    "# Function for finding the betweenness centrality of a vertex\n",
    "def betweenness_centrality(graph, vertex):\n",
    "    vals = []\n",
    "    b_cent = 0\n",
    "    size = number_of_vertices(graph)\n",
    "    graph = create_graph(graph)\n",
    "\n",
    "    if graph[0][0] == 1:\n",
    "        vertecies = list(range(1, 1 + size))\n",
    "    else:\n",
    "        vertecies = list(range(0, size))\n",
    "\n",
    "    vertecies.remove(vertex)\n",
    "\n",
    "    # list of pair values.\n",
    "    for s in range(len(vertecies)):\n",
    "        for t in range(len(vertecies)):\n",
    "            if s != t:\n",
    "                vals = vals + [[vertecies[s], vertecies[t]]]\n",
    "\n",
    "    # no duplicates.\n",
    "    s = set()\n",
    "    out = []\n",
    "    for i in vals:\n",
    "        t = tuple(i)\n",
    "        if t in s or tuple(reversed(t)) in s:\n",
    "            continue\n",
    "        s.add(t)\n",
    "        out.append(i)\n",
    "\n",
    "    x_part = [i[0] for i in out]\n",
    "    y_part = [i[1] for i in out]\n",
    "\n",
    "    # finding shortest paths\n",
    "    for (x, y) in zip(x_part, y_part):\n",
    "        counter = 0\n",
    "\n",
    "        # Get a list of all the shortest paths for every pair of nodes\n",
    "        list_shortest_paths = list([p for p in nx.all_shortest_paths(graph, source=x, target=y)])\n",
    "\n",
    "        # Get the total number of shortest paths\n",
    "        number_of_shortest_path = len(list_shortest_paths)\n",
    "\n",
    "        # Flatten the nested list into a list of single elements\n",
    "        flat_list = [item for sublist in list_shortest_paths for item in sublist]\n",
    "\n",
    "        # counting occurances of betweeness of a node\n",
    "        for i in flat_list:\n",
    "            if i == vertex:\n",
    "                counter = counter + 1\n",
    "\n",
    "        # Result\n",
    "        b_cent = b_cent + (counter / number_of_shortest_path)\n",
    "    return b_cent\n",
    "\n",
    "\n",
    "def adj_matrix(graph):\n",
    "    vertices = number_of_vertices(graph)\n",
    "    # Initilize empty array\n",
    "    matrix_array = np.zeros((vertices, vertices))\n",
    "    # Use the vertices as coordinate to iterate thru the array\n",
    "    x_part = [i[0]-1 for i in graph]\n",
    "    y_part = [i[1]-1 for i in graph]\n",
    "    # modifies the matrix.\n",
    "    for (x, y) in zip(x_part, y_part):\n",
    "        matrix_array[x][y] = 1\n",
    "        matrix_array[y][x] = 1\n",
    "    return matrix_array\n",
    "\n",
    "\n",
    "def prestiege_centrality(matrix_arrr):\n",
    "    current_vector = np.ones(len(matrix_arrr))\n",
    "    previous_vector = np.ones(len(matrix_arrr))\n",
    "    result = np.ones(len(matrix_arrr))\n",
    "    for i in range(100):\n",
    "        previous_vector = current_vector\n",
    "        current_vector = np.dot(matrix_arrr, previous_vector)\n",
    "        result = np.dot(current_vector, (1/np.linalg.norm(current_vector, 2)))\n",
    "    return result\n",
    "\n",
    "\n",
    "# Citations\n",
    "# https://www.geeksforgeeks.org/python-find-the-tuples-containing-the-given-element-from-a-list-of-tuples\n",
    "# https://networkx.org/documentation/\n",
    "# https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "# https://towardsdatascience.com/customizing-networkx-graphs-f80b4e69bedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get distance between 2 points\n",
    "def distance(mean_point, distance_point):\n",
    "    return math.sqrt(sum([(mean_point[index] - distance_point[index]) ** 2 for index in range(len(mean_point))]))\n",
    "\n",
    "\n",
    "#Return if in neighborhood\n",
    "def within_neighborhood(point_1, point_2, max_distance):\n",
    "    return distance(point_1, point_2) <= max_distance\n",
    "\n",
    "#Create Random starting values for kmeans\n",
    "def randomize(ranges, k):\n",
    "    output = []\n",
    "    while k > 0:\n",
    "        output.append(tuple([random.uniform(ranges[column][0], ranges[column][1]) for column in range(len(ranges))]))\n",
    "        k -= 1\n",
    "    return output\n",
    "\n",
    "\n",
    "#Find mean based on index subset\n",
    "def mean(matrix, indexes):\n",
    "    array = list(zip(*[list(matrix[index, :]) for index in indexes]))\n",
    "    array = [list(item) for item in array]\n",
    "    return tuple([sum(index)/len(index) for index in array])\n",
    "\n",
    "#Return index of closest point in cluster points to point(used to find closest cluster)\n",
    "def closest(cluster_points, point):\n",
    "    distances = [distance(cluster_point, point) for cluster_point in cluster_points]\n",
    "    return distances.index(min(distances))\n",
    "\n",
    "#Uses Kmeans Algorithm to return cluster points and matrix row indexes in each cluster\n",
    "def kmeans(matrix, k = 2, e = 10000):\n",
    "    ranges = [(min(matrix[:,column]), max(matrix[:,column]))  for column in range(len(matrix[0]))]\n",
    "    start_points = []\n",
    "    end_points = randomize(ranges, k)\n",
    "    clusters = []\n",
    "    while start_points != end_points and e > 0:\n",
    "        start_points = end_points\n",
    "        clusters = [[] for index in range(k)]\n",
    "        for row in range(len(matrix)):\n",
    "            clusters[closest(start_points, tuple(matrix[row]))].append(row)\n",
    "        end_points = [mean(matrix, item) for item in clusters]\n",
    "        e -= 1\n",
    "    return end_points, clusters\n",
    "\n",
    "\n",
    "class Point():\n",
    "    #Point is a tuple of point values\n",
    "    def __init__(self, point):\n",
    "        self.point = point\n",
    "        self.label =  \"noise\"\n",
    "        self.cluster = []\n",
    "\n",
    "    def get_cluster(self, cluster = [], loop = 0):\n",
    "        length = len(cluster)\n",
    "        new = [self] + self.cluster\n",
    "        cluster = cluster + new\n",
    "        cluster = list(set(cluster))\n",
    "        if len(cluster) == length:\n",
    "            return cluster\n",
    "        for item in new:\n",
    "            cluster = item.get_cluster(cluster = cluster, loop = loop+1)\n",
    "        return cluster\n",
    "\n",
    "\n",
    "def dbscan(matrix, minpts = 5, e = 10000):\n",
    "    points = [Point(tuple(matrix[row,:])) for row in range(len(matrix))]\n",
    "    for item in points:\n",
    "        for other in points:\n",
    "            if within_neighborhood(item.point, other.point, minpts):\n",
    "                item.cluster.append(other)\n",
    "                other.cluster.append(item)\n",
    "    for item in points:\n",
    "        item.cluster = list(set(item.cluster))\n",
    "        if len(item.cluster) >= minpts:\n",
    "            item.label = \"core\"\n",
    "        else:\n",
    "            for other in item.cluster:\n",
    "                if other.label == \"core\":\n",
    "                    item.label = \"border\"\n",
    "                    break\n",
    "    output =  list(set([tuple(point.get_cluster()) for point in points]))\n",
    "    return [list(item) for item in output]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "\n",
    "# Report\n",
    "## What problem were you trying to solve or help solve?\n",
    "\n",
    "\n",
    "## Describe the data:\n",
    "- How many instances?  \n",
    "There are 270 instances\n",
    "- How many attributes?  \n",
    "There are 13 attributes\n",
    "- Any missing Values?  \n",
    "There are no missing values.\n",
    "- Number of categorical and numeric attributes?  \n",
    "The 13 attributes (which have been extracted from a larger set of 75) include descriptive variables such as sex, and Chest pain type (4 values), however, these values were already label encoded. As such, we are left with the following attribute types: Real: 1,4,5,8,10,12 Ordered:11, Binary: 2,6,9 Nominal:7,3,13\n",
    "\n",
    "## What pre-processing techniques did you apply and when? Make sure to justify the use of each technique you used. For example label vs. one-hot encoding.\n",
    "There are no preprocessing techniques that we needed. The data was already label-encoded therefore we did not need to do that.\n",
    "\n",
    "## What data mining techniques did you apply and why? Mkae sure to justify the use of each technique you used. For example, why did you use k-means instead of DBSCAN.\n",
    "We are choosing DBSCAN because we want to accurately identify the outliers.\n",
    "\n",
    "\n",
    "## Include relevant visualizations and tables summarizing your data and your findings. This may include:\n",
    "- a table listing the number attributes, missing values, number of classes, parameter set-tings, etc.\n",
    "- visualization of a large graph if you are working with graph data.\n",
    "- one or more visualizations of your data in two dimensions (original dimensions or PCA dimensions).\n",
    "- for PCA, a plot of r vs. f(r).\n",
    "- for k-means, a plot of the objective function for various k‚Äôs.\n",
    "- for DBSCAN, a plot or table of the precision at various parameters.\n",
    "- other visualizations or tables that you think will effectively communicate your ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What did you learn through your analysis?\n",
    "\n",
    "\n",
    "## Was anything about your results surprising or unexpected?\n",
    "\n",
    "\n",
    "\n",
    "## How will your work help with understanding the problem you set out to solve?\n",
    "\n",
    "\n",
    "## What else would you do if you had more time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "\n",
    "# Present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a 5-10 minute video presentation summarizing your findings. You may use whatever video  \n",
    "editing technology you prefer. (The MSU supported tool is TechSmith Relay. See the UIT tutorial for more info.) The video should:  \n",
    "‚Ä¢ State your name.  \n",
    "‚Ä¢ Summarize your project, including:  \n",
    "‚Äì the problem you are interested in.  \n",
    "‚Äì what data mining techniques you used to analyze data related to the problem.  \n",
    "‚Ä¢ Your key findings and any surprising results.  \n",
    "‚Ä¢ What else you would work on if you had more time.  \n",
    "The goal is to summarize the work you have done and what you have learned from the process.  \n",
    "Note: any presentation that exceeds 10 minutes or does not reach 5 minutes will be docked 1 point per minute.  "
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Michael Roduin"
   },
   {
    "name": "Moiyad Alfawwar"
   },
   {
    "name": "Philip Gehde"
   }
  ],
  "interpreter": {
   "hash": "435ca20ebd40facca939da61bf2b33486b57b976545d40e14cf63d5fdaaad0da"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('csci347')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "title": "CSCI 347: Project 4"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
