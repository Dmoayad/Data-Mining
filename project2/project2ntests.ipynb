{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 347 Project 2\n",
    "Philip Gehde and Moiyad Alfawwar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: Think about the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the LastFM Asia Social network. This was a good network to practice on because of its relatively small size with 7,624 Nodes,\n",
    "and 27,806 Edges. The relationship between users appeared interesting to us, as we could use our analysis to evaluate similar networks. We use social networks on a regular basis, so understanding the context of the data would presumably help. The fact that the dataset was undirected, and not temporal, made our work and choice of network just a little bit easier.\n",
    "We choose not to preprocess the data, because of its small size. \n",
    "The network is said to have a low density of 0.001, which makes sense if we think about the amount of actors in the network compared to the relationships between them. In a social network an actor would only have a select few relationships compared to the overall user-base. Thus, we\n",
    "believe that vertices of high centrality are relatively few and far in between. \n",
    "We would consider these vertices to represent accounts of high influence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2: Write Python code for graph analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "# import csv\n",
    "# !pip install scipy\n",
    "from scipy import *\n",
    "\n",
    "\n",
    "# # read data from the csv file\n",
    "# def read_dat()\n",
    "\n",
    "# 2.1 Function to determine number of vertices in a graph\n",
    "def number_of_vertices(graph):\n",
    "    # tuples to list\n",
    "    my_list = [item for x in graph for item in x]\n",
    "    # remove duplicates\n",
    "    unique_set = set(my_list)\n",
    "    counter = len(unique_set)\n",
    "    return counter\n",
    "\n",
    "\n",
    "# 2.2 Function for finding the degree of a vertex\n",
    "def degrees_of_vertex(graph, vertex):\n",
    "    # Converting tuples to list\n",
    "    my_list = [item for x in graph for item in x]\n",
    "    counter = 0\n",
    "    for i in my_list:\n",
    "        if i == vertex:\n",
    "            counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "# Helper function: Get number of edges from subgraph\n",
    "def subgraph_edges(graph, vertices):\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(graph)\n",
    "    induced_subgraph = G.subgraph(vertices)\n",
    "    number_of_edges = nx.number_of_edges(induced_subgraph)\n",
    "    return number_of_edges\n",
    "\n",
    "\n",
    "# Helper function: Generate edges_list in graph\n",
    "def generate_graph(edges):\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges)\n",
    "    return G\n",
    "\n",
    "# Function for finding the clustering coefficient of a vertex\n",
    "# ref: https://www.geeksforgeeks.org/python-find-the-tuples-containing-the-given-element-from-a-list-of-tuples/1\n",
    "def clustering_coefficient(graph, vertex):\n",
    "    # Obtains a list of all the neighbors of the given vertex\n",
    "    filtered_list = list(filter(lambda x: vertex in x, graph))\n",
    "    # Converts the list of tuples into a regular list\n",
    "    regular_list = [item for x in filtered_list for item in x]\n",
    "    # Remove the vertex value from the regular list to get the list of just the neighbors\n",
    "    neighbors = list(filter(vertex.__ne__, regular_list))\n",
    "    # Calls a helper function that generates a graph from the edge list and creates a subgraph\n",
    "    # of neighbors and then finds the number of edges amongst the subgraph of neighbors\n",
    "    total_edges_actual = subgraph_edges(neighbors, graph)\n",
    "    # Gets the number of neighbors\n",
    "    number_of_neighbors = len(neighbors)\n",
    "    # Calculates the number of total edges possible by (n-1)*(n/2) where n is the number of vertices\n",
    "    total_edges_possible = (number_of_neighbors - 1) * (number_of_neighbors / 2)\n",
    "    # Calculates the clustering coefficient by dividing\n",
    "    # the number of actual edges by the total number of possible edges\n",
    "    clustering_coefficient = total_edges_actual / total_edges_possible\n",
    "    return clustering_coefficient\n",
    "\n",
    "# Function for finding the betweenness centrality of a vertex\n",
    "def betweenness_centrality(edgelist, vertex):\n",
    "    values = []\n",
    "    betweenness = 0\n",
    "    size = number_of_vertices(edgelist)\n",
    "    # Generate a graph from the given edgelist using a helper function\n",
    "    graph = generate_graph(edgelist)\n",
    "\n",
    "    # To determine if edgelist starts with 0 vs 1\n",
    "    if edgelist[0][0] == 1:\n",
    "        vertex_list = list(range(1, 1 + size))\n",
    "    else:\n",
    "        vertex_list = list(range(0, size))\n",
    "\n",
    "    # Removes the specified vertex from the vertex list since it\n",
    "    # won't be used in making a list of all possible vertex pairs\n",
    "    vertex_list.remove(vertex)\n",
    "\n",
    "    # Create a list of all the vertex pairs excluding x,x pairs (i.e. 1,1 or 2,2 etc.)\n",
    "    for s in range(len(vertex_list)):\n",
    "        for t in range(len(vertex_list)):\n",
    "            if s != t:\n",
    "                values = values + [[vertex_list[s], vertex_list[t]]]\n",
    "\n",
    "    # Get rid of mirrored duplicates (i.e. [[0,1],[1,0]] -> [[0,1]])\n",
    "    s = set()\n",
    "    out = []\n",
    "    for i in values:\n",
    "        t = tuple(i)\n",
    "        if t in s or tuple(reversed(t)) in s:\n",
    "            continue\n",
    "        s.add(t)\n",
    "        out.append(i)\n",
    "\n",
    "    # Extract the columns from the nested list so we have a list of every\n",
    "    # possible node pair with no repeats and no x, x node pairs\n",
    "    x_bar = [i[0] for i in out]\n",
    "    y_bar = [i[1] for i in out]\n",
    "\n",
    "    # Simultaneously iterate through the separate columns representing all vertex pairs and find all shortest paths\n",
    "    for (x, y) in zip(x_bar, y_bar):\n",
    "        count = 0\n",
    "\n",
    "        # Get a list of all the shortest paths for every pair of nodes\n",
    "        list_shortest_paths = list([p for p in nx.all_shortest_paths(graph, source=x, target=y)])\n",
    "\n",
    "        # Get the total number of shortest paths\n",
    "        number_of_shortest_path = len(list_shortest_paths)\n",
    "\n",
    "        # Flatten the nested list into a list of single elements\n",
    "        flat_list = [item for sublist in list_shortest_paths for item in sublist]\n",
    "\n",
    "        # Search for the number of occurrences of the betweenness node\n",
    "        for i in flat_list:\n",
    "            if i == vertex:\n",
    "                count = count + 1\n",
    "\n",
    "        # Calculate betweenness centrality\n",
    "        betweenness = betweenness + (count / number_of_shortest_path)\n",
    "    return betweenness\n",
    "\n",
    "\n",
    "def adj_matrix(graph):\n",
    "    vertices = number_of_vertices(graph)\n",
    "    # Initilize empty array\n",
    "    matrix_array = np.zeros((vertices, vertices))\n",
    "    # Use the vertices as coordinate to iterate thru the array\n",
    "    x_part = [i[0]-1 for i in graph]\n",
    "    y_part = [i[1]-1 for i in graph]\n",
    "\n",
    "    # modifies the matrix.\n",
    "    for (x, y) in zip(x_part, y_part):\n",
    "        matrix_array[x][y] = 1\n",
    "        matrix_array[y][x] = 1\n",
    "    return matrix_array\n",
    "\n",
    "\n",
    "def prestiege_centrality(matrix_arrr):\n",
    "    current_vector = np.ones(len(matrix_arrr))\n",
    "    previous_vector = np.ones(len(matrix_arrr))\n",
    "    result = np.ones(len(matrix_arrr))\n",
    "    for i in range(100):\n",
    "        previous_vector = current_vector\n",
    "        current_vector = np.dot(matrix_arrr, previous_vector)\n",
    "        result = np.dot(current_vector, (1/np.linalg.norm(current_vector, 2)))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case graphs for Project 2: CSCI 347\n",
    "\n",
    "g0 = nx.Graph()\n",
    "g0.add_edge(1,2)\n",
    "g0.add_edge(2,3)\n",
    "g0.add_edge(2,4)\n",
    "g0.add_edge(3,5)\n",
    "g0.add_edge(4,5)\n",
    "g0.add_edge(5,6)\n",
    "\n",
    "graph_0 = [(1,2), (2,3), (2,4), (3,5),(4,5),(5,6)]\n",
    "\n",
    "graph_1 = [(1,2), (2,3), (3,4), (4,5),(5,1)]\n",
    "g1 = nx.Graph()\n",
    "for edge in graph_1:\n",
    "    g1.add_edge(edge[0], edge[1])\n",
    "\n",
    "graph_2 = [(1,2),(1,3), (2,3),(2,4) ,(3,4), (4,5),(5,1)]\n",
    "g2 = nx.Graph()\n",
    "for edge in graph_2:\n",
    "    g2.add_edge(edge[0], edge[1])\n",
    "\n",
    "graph_3 = [(1,2),(1,3), (2,3),(2,4) ,(3,4),(3,6), (4,5),(5,1),(6,7),(6,8), (7,8), (8,9) ]\n",
    "g3 = nx.Graph()\n",
    "for edge in graph_3:\n",
    "    g3.add_edge(edge[0], edge[1])\n",
    "\n",
    "g4 = nx.path_graph(50)\n",
    "graph_4 = []\n",
    "for line in nx.generate_edgelist(g4, data=False):\n",
    "    vi = int(line.split()[0])\n",
    "    vj = int(line.split()[1])\n",
    "    pair = (vi, vj)\n",
    "    graph_4.append(pair)\n",
    "\n",
    "g5 = nx.complete_graph(7)\n",
    "graph_5 = []\n",
    "for line in nx.generate_edgelist(g5, data=False):\n",
    "    vi = int(line.split()[0])\n",
    "    vj = int(line.split()[1])\n",
    "    pair = (vi, vj)\n",
    "    graph_5.append(pair)\n",
    "\n",
    "g6 = nx.cycle_graph(8)\n",
    "graph_6 = []\n",
    "for line in nx.generate_edgelist(g6, data=False):\n",
    "    vi = int(line.split()[0])\n",
    "    vj = int(line.split()[1])\n",
    "    pair = (vi, vj)\n",
    "    graph_6.append(pair)\n",
    "\n",
    "n=200 # number of nodes for Barabasi-Albert graph\n",
    "q=4 # number of edges each new node has in the generation process of BA graph\n",
    "\n",
    "g7 = nx.barabasi_albert_graph(n, q, seed=34)\n",
    "graph_7 = []\n",
    "for line in nx.generate_edgelist(g7, data=False):\n",
    "    vi = int(line.split()[0])\n",
    "    vj = int(line.split()[1])\n",
    "    pair = (vi, vj)\n",
    "    graph_7.append(pair)\n",
    "\n",
    "\n",
    "g8 = nx.erdos_renyi_graph(100,0.13, seed=15)\n",
    "graph_8 = []\n",
    "for edge in g8.edges():\n",
    "    graph_8.append(edge)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # number_of_vertices(graph_8)\n",
    "# # degrees_of_vertex(graph_7, 27) \n",
    "# # graph_7.degree(27)\n",
    "# G = nx.Graph()\n",
    "# # G.add_edge(graph_7[0])\n",
    "# gg = graph_3\n",
    "# for i in range(number_of_vertices(gg)):\n",
    "#     G.add_edge(gg[i][0],gg[i][1])\n",
    "\n",
    "# # nx.betweenness_centrality(G)[23]\n",
    "\n",
    "# # degrees_of_vertex(graph_3, 6) \n",
    "# # graph_7[0][1]\n",
    "# # graph_7[0]\n",
    "# # clustering_coefficient(graph_8, 46)\n",
    "\n",
    "# # nx.average_shortest_path_length(G)\n",
    "# # print(adjacency_matrix(graph_1))\n",
    "# # betweenness_centrality(graph_8, 39)\n",
    "# # nx.eigenvector_centrality(G)\n",
    "# # nx.pagerank(G)\n",
    "# # prestiege_centrality(adj_matrix(graph_1))\n",
    "# arr2 = adj_matrix(graph_0)\n",
    "# # arr = np.ones((6,1))\n",
    "# # np.dot(arr2, arr)\n",
    "# arr2\n",
    "# print(prestiege_centrality(arr2))\n",
    "# # nx.eigenvector_centrality(G)\n",
    "# # pos=nx.spring_layout(G)\n",
    "# # nx.draw_networkx_edges(G,pos,width=1.0,alpha=0.5)\n",
    "# # nx.draw_networkx_nodes(G, pos, alpha=0.7)\n",
    "# # nx.draw(G, linewidths=1, edgecolors=\"red\")\n",
    "# sorted(G.degree, key=lambda x: x[1], reverse=True)\n",
    "# dict = {k: v for k, v in sorted(nx.betweenness_centrality(G).items(), key=lambda item: item[1], reverse=True)}\n",
    "# {k:v for (k,v) in [x for x in dict.items()][:10]}\n",
    "\n",
    "# highest_clustering = {k: v for k, v in sorted(nx.clustering(G).items(), key=lambda item: item[1], reverse=True)}\n",
    "# {k:v for (k,v) in [x for x in highest_clustering.items()][:10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Analyze the graph data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Produce a visualization of the graph (or graph sample that you used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist(\"data\\lastfm_asia_edges.csv\", delimiter=\",\")\n",
    "# nx.draw_networkx(G, with_labels=False, node_size=100, edgecolors=\"#202020\", alpha=0.7)\n",
    "# G.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the 10 nodes with the highest degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(nx.degree(G), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find the 10 nodes with the highest betweenness centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_betweenness_cent = {k: v for k, v in sorted(nx.betweenness_centrality(G).items(), key=lambda item: item[1], reverse=True)}\n",
    "{k:v for (k,v) in [x for x in highest_betweenness_cent.items()][:10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Find the 10 nodes with the highest clustering coefficient. If there are ties, choose 10 to report and explain how the 10 were chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_clustering = {k: v for k, v in sorted(nx.clustering(G).items(), key=lambda item: item[1], reverse=True)}\n",
    "{k:v for (k,v) in [x for x in highest_clustering.items()][:10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Find the top 10 nodes as ranked by prestige centrality (eigenvector centrality in networkx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_prestige_cent = {k: v for k, v in sorted(nx.eigenvector_centrality(G).items(), key=lambda item: item[1], reverse=True)}\n",
    "{k:v for (k,v) in [x for x in highest_prestige_cent.items()][:10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Find the top 10 nodes as ranked by Pagerank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_pgrank = {k: v for k, v in sorted(nx.pagerank(G).items(), key=lambda item: item[1], reverse=True)}\n",
    "{k:v for (k,v) in [x for x in highest_pgrank.items()][:10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. (3 points) Comment on the differences and similarities in questions Part 3 1-6. Are the highly ranked nodes mostly the same? Do you notice significant differences in the rankings? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}